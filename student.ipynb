{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f16c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import gc # Garbage collection\n",
    "from datetime import datetime\n",
    "import os \n",
    "\n",
    "\n",
    "# sklearn.model_selection.StratifiedKFold: For stratified k-fold cross-validation\n",
    "# sklearn.preprocessing: Tools for data preprocessing (LabelEncoder, StandardScaler)\n",
    "# sklearn.metrics.roc_auc_score: For calculating area under ROC curve\n",
    "# lightgbm (lgb): Gradient boosting framework optimized for efficiency and performance\n",
    "# tqdm: Provides progress bars for loops\n",
    "# gc: Garbage collection for memory management\n",
    "# datetime: For working with dates and times\n",
    "# os: For interacting with the operating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e15ec202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Imports - PyTorch\n",
    "# Ensure PyTorch is installed: pip install torch torchvision torchaudio\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader,Dataset\n",
    "    PYTORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"PyTorch not found. Deep Learning part will be skipped. Install PyTorch (torch) to enable it.\")\n",
    "    PYTORCH_AVAILABLE = False\n",
    "\n",
    "    # torch: Main PyTorch package providing tensor computations and automatic differentiation\n",
    "    # torch.nn: Neural network module containing layers, activation functions, and loss functions\n",
    "    # torch.optim: Package implementing various optimization algorithms (SGD, Adam, etc.)\n",
    "    # torch.utils.data.DataLoader: Utility for batch loading, shuffling, and parallel data processing\n",
    "    # torch.utils.data.TensorDataset: Simple dataset class wrapping tensors\n",
    "    # torch.utils.data.Dataset: Abstract class representing a dataset for creating custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c60a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DATA_PATH = '.' # <<< --- USER: PLEASE VERIFY THIS PATH ---\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"ERROR: Data path '{DATA_PATH}' does not exist. Please update the DATA_PATH variable.\")\n",
    "    \n",
    "USER_LOG_FILE = os.path.join(DATA_PATH, 'user_log_format1.csv')\n",
    "USER_INFO_FILE = os.path.join(DATA_PATH, 'user_info_format1.csv')\n",
    "TRAIN_FILE = os.path.join(DATA_PATH, 'train_format1.csv')\n",
    "TEST_FILE = os.path.join(DATA_PATH, 'test_format1.csv')\n",
    "SUBMISSION_FILE = 'prediction_pytorch_lgbm.csv'\n",
    "DL_MODEL_CHECKPOINT_PATH = 'best_dl_model_fold_{fold}.pth' # PyTorch model extension\n",
    "\n",
    "D11_MONTH = 11\n",
    "D11_DAY = 11\n",
    "D11_TIMESTAMP_INT = D11_MONTH * 100 + D11_DAY # 1111 represents 11/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f4c2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions ---\n",
    "def convert_mmdd_to_days_before_d11(mmdd_series, ref_month=D11_MONTH, ref_day=D11_DAY):\n",
    "    days_in_month_cumulative = [0, 0, 31, 31+28, 31+28+31, 31+28+31+30, 31+28+31+30+31, \n",
    "                               31+28+31+30+31+30, 31+28+31+30+31+30+31, 31+28+31+30+31+30+31+31,\n",
    "                               31+28+31+30+31+30+31+31+30, 31+28+31+30+31+30+31+31+30+31,\n",
    "                               31+28+31+30+31+30+31+31+30+31+30]\n",
    "    \n",
    "    def date_to_day_of_year(mmdd_str):\n",
    "        if pd.isna(mmdd_str) or not isinstance(mmdd_str, str) or len(mmdd_str) != 4:\n",
    "            return np.nan\n",
    "        try:\n",
    "            m = int(mmdd_str[:2])\n",
    "            d = int(mmdd_str[2:])\n",
    "            if not (1 <= m <= 12 and 1 <= d <= 31): # Basic validation\n",
    "                return np.nan\n",
    "            return days_in_month_cumulative[m] + d\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    ref_day_of_year = date_to_day_of_year(f\"{ref_month:02d}{ref_day:02d}\")\n",
    "    if pd.isna(ref_day_of_year):\n",
    "        raise ValueError(\"Reference date (D11) is invalid.\")\n",
    "        \n",
    "    day_of_year_series = mmdd_series.apply(date_to_day_of_year)\n",
    "    return ref_day_of_year - day_of_year_series\n",
    "\n",
    "    # Function: convert_mmdd_to_days_before_d11\n",
    "    # \n",
    "    # Converts dates from 'mmdd' string format to the number of days before D11 (November 11)\n",
    "    # \n",
    "    # Parameters:\n",
    "    # - mmdd_series: Series of strings in 'mmdd' format (e.g., '1101' for November 1)\n",
    "    # - ref_month: Reference month (default: D11_MONTH which is 11 for November)\n",
    "    # - ref_day: Reference day (default: D11_DAY which is 11)\n",
    "    # \n",
    "    # Returns:\n",
    "    # - Series with the number of days between each date and the reference date (D11)\n",
    "    # - Positive values indicate dates before D11, negative values are after D11\n",
    "    # - NaN values for invalid date formats or dates\n",
    "    #\n",
    "    # Note: Uses day-of-year calculation based on a non-leap year calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fdc5680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading and Basic Preprocessing ---\n",
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    # Processing Steps:\n",
    "    #   1. Loads all CSV files with optimized data types to reduce memory usage\n",
    "    user_log_dtypes = {'user_id': np.uint32, 'item_id': np.uint32, \n",
    "                       'cat_id': np.uint16, 'seller_id': np.uint16, \n",
    "                       'brand_id': str, 'time_stamp': str, 'action_type': np.uint8}\n",
    "    user_info_dtypes = {'user_id': np.uint32, 'age_range': str, 'gender': str} \n",
    "    train_dtypes = {'user_id': np.uint32, 'merchant_id': np.uint16, 'label': np.uint8}\n",
    "    test_dtypes = {'user_id': np.uint32, 'merchant_id': np.uint16}\n",
    "\n",
    "    try:\n",
    "        user_log = pd.read_csv(USER_LOG_FILE, dtype=user_log_dtypes)\n",
    "        user_info = pd.read_csv(USER_INFO_FILE, dtype=user_info_dtypes)\n",
    "        train_data = pd.read_csv(TRAIN_FILE, dtype=train_dtypes)\n",
    "        test_data = pd.read_csv(TEST_FILE, dtype=test_dtypes)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: File not found. {e}. Please check your DATA_PATH ('{DATA_PATH}') and file names.\")\n",
    "        raise\n",
    "    #   2. Adds a placeholder 'prob' column to test data for later predictions    \n",
    "    test_data['prob'] = 0.0 \n",
    "\n",
    "    print(\"Preprocessing basic data...\")\n",
    "    #   3. Renames 'seller_id' to 'merchant_id' in user_log for consistency\n",
    "    user_log.rename(columns={'seller_id': 'merchant_id'}, inplace=True)\n",
    "    #   4. Converts demographic variables (age_range, gender) to numeric types\n",
    "    user_info['age_range'] = pd.to_numeric(user_info['age_range'], errors='coerce').fillna(0).astype(np.uint8)\n",
    "    user_info['gender'] = pd.to_numeric(user_info['gender'], errors='coerce').fillna(2).astype(np.uint8)\n",
    "    #   5. Processes timestamps and calculates days relative to the D11 event (Nov 11)\n",
    "    user_log['time_stamp_int'] = user_log['time_stamp'].apply(\n",
    "        lambda x: int(x) if pd.notna(x) and isinstance(x, str) and x.isdigit() and len(x) == 4 else -1\n",
    "    )\n",
    "    user_log['days_before_d11'] = convert_mmdd_to_days_before_d11(user_log['time_stamp'])\n",
    "    #   6. Creates a flag for records occurring on D11 (the major shopping day)\n",
    "    user_log['is_d11'] = (user_log['time_stamp_int'] == D11_TIMESTAMP_INT).astype(np.uint8)\n",
    "    #   7. Cleans and converts brand_id to numeric format\n",
    "    user_log['brand_id'] = pd.to_numeric(user_log['brand_id'], errors='coerce').fillna(0).astype(np.uint32)\n",
    "    #   8. Sorts user logs chronologically for each user\n",
    "    user_log.sort_values(by=['user_id', 'time_stamp_int'], ascending=[True, True], inplace=True)\n",
    "    \n",
    "    print(\"Data loaded and basic preprocessing done.\")\n",
    "    return user_log, user_info, train_data, test_data\n",
    "\n",
    "    # load_data() Function Explanation\n",
    "    # \n",
    "    # Purpose: \n",
    "    #   Loads and performs initial preprocessing of all datasets needed for the analysis.\n",
    "    #\n",
    "    # Data Files:\n",
    "    #   - USER_LOG_FILE: Contains user interaction logs (clicks, purchases, etc.)\n",
    "    #   - USER_INFO_FILE: Contains demographic information about users\n",
    "    #   - TRAIN_FILE: Contains training data pairs (user_id, merchant_id) with purchase labels\n",
    "    #   - TEST_FILE: Contains test data pairs (user_id, merchant_id) for prediction\n",
    "    #\n",
    "    #\n",
    "    # Returns:\n",
    "    #   - user_log: Processed user activity log data\n",
    "    #   - user_info: Processed user demographic data\n",
    "    #   - train_data: Labeled user-merchant pairs for model training\n",
    "    #   - test_data: User-merchant pairs for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cfe620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition script started at: 2025-05-11 22:48:42.836280\n",
      "Loading data...\n",
      "Preprocessing basic data...\n",
      "Data loaded and basic preprocessing done.\n",
      "Starting comprehensive feature engineering...\n",
      "Engineering user-level features...\n",
      "Engineering merchant-level features...\n",
      "Engineering user-merchant interaction features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yishu\\AppData\\Local\\Temp\\ipykernel_61768\\1505742554.py:130: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  else: merged_df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\yishu\\AppData\\Local\\Temp\\ipykernel_61768\\1505742554.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  if 'days_prior' in col or 'score' in col: merged_df[col].fillna(-1, inplace=True)\n",
      "C:\\Users\\yishu\\AppData\\Local\\Temp\\ipykernel_61768\\1505742554.py:129: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  elif col in ['acq_item_id', 'acq_cat_id', 'acq_brand_id']: merged_df[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete.\n",
      "Train featured shape: (260864, 89)\n",
      "Test featured shape: (261477, 89)\n",
      "Identified 6 categorical features for DL embeddings: ['u_age_range', 'u_gender', 'merchant_id', 'acq_item_id', 'acq_cat_id', 'acq_brand_id']\n",
      "Identified 81 numerical features for DL: ['u_hist_total_actions', 'u_hist_n_distinct_items', 'u_hist_n_distinct_categories', 'u_hist_n_distinct_merchants', 'u_hist_n_distinct_brands', 'u_hist_days_active', 'u_hist_earliest_action_days_prior', 'u_hist_latest_action_days_prior', 'u_hist_mean_action_days_prior', 'u_hist_std_action_days_prior']...\n",
      "Using 87 features for LGBM training.\n",
      "Training LightGBM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LGBM Folds: 100%|██████████| 5/5 [00:29<00:00,  5.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM OOF AUC: 0.68686\n",
      "\n",
      "LGBM Top 30 Feature Importances (Mean over folds):\n",
      "                                            mean_importance\n",
      "acq_item_id                                          1337.2\n",
      "merchant_id                                          1023.4\n",
      "acq_brand_id                                          927.6\n",
      "acq_cat_id                                            635.2\n",
      "ratio_um_d11_actions_vs_m_d11_interactions            598.0\n",
      "ratio_um_d11_actions_vs_u_d11_actions                 313.4\n",
      "um_hist_first_interaction_days_prior                  287.8\n",
      "u_hist_purchase_to_click_ratio                        273.0\n",
      "u_hist_mean_action_days_prior                         258.6\n",
      "u_hist_std_action_days_prior                          246.6\n",
      "m_d11_action_type_2_count                             230.8\n",
      "u_hist_purchase_to_fav_ratio                          216.4\n",
      "um_d11_distinct_items_interacted                      213.8\n",
      "um_d11_purchases                                      210.8\n",
      "u_hist_earliest_action_days_prior                     183.6\n",
      "u_hist_purchase_to_cart_ratio                         182.6\n",
      "ratio_um_hist_actions_vs_u_hist_actions               180.6\n",
      "u_hist_days_active                                    163.2\n",
      "u_d11_n_distinct_merchants                            161.6\n",
      "u_purchases_last_180d_prior                           160.2\n",
      "m_d11_total_interactions                              155.2\n",
      "um_hist_distinct_items                                153.2\n",
      "u_d11_total_actions                                   148.4\n",
      "m_d11_n_distinct_users                                146.6\n",
      "u_hist_action_type_2_count                            142.4\n",
      "ratio_um_d11_purchases_vs_u_d11_purchases             142.2\n",
      "u_d11_action_type_0_count                             141.4\n",
      "u_actions_last_90d_prior                              136.8\n",
      "um_d11_total_actions                                  130.4\n",
      "u_hist_action_type_3_count                            127.4\n",
      "Preparing data for PyTorch Deep Learning model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label Encoding DL Categoricals:  17%|█▋        | 1/6 [00:00<00:00,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DL Cat Feature: u_age_range, Input Dim: 9, Output Dim: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label Encoding DL Categoricals:  33%|███▎      | 2/6 [00:00<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DL Cat Feature: u_gender, Input Dim: 3, Output Dim: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label Encoding DL Categoricals:  50%|█████     | 3/6 [00:00<00:00,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DL Cat Feature: merchant_id, Input Dim: 1994, Output Dim: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label Encoding DL Categoricals:  83%|████████▎ | 5/6 [00:00<00:00,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DL Cat Feature: acq_item_id, Input Dim: 71478, Output Dim: 50\n",
      "  DL Cat Feature: acq_cat_id, Input Dim: 966, Output Dim: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label Encoding DL Categoricals: 100%|██████████| 6/6 [00:01<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DL Cat Feature: acq_brand_id, Input Dim: 2865, Output Dim: 50\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DL Folds:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DL Fold 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yishu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 0.2880 - Val AUC: 0.6588 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6588, model saved.\n",
      "Epoch 2/30 - Train Loss: 0.2266 - Val AUC: 0.6677 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6677, model saved.\n",
      "Epoch 3/30 - Train Loss: 0.2230 - Val AUC: 0.6731 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6731, model saved.\n",
      "Epoch 4/30 - Train Loss: 0.2208 - Val AUC: 0.6798 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6798, model saved.\n",
      "Epoch 5/30 - Train Loss: 0.2194 - Val AUC: 0.6824 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6824, model saved.\n",
      "Epoch 6/30 - Train Loss: 0.2176 - Val AUC: 0.6780 - LR: 1.0e-03\n",
      "Epoch 7/30 - Train Loss: 0.2164 - Val AUC: 0.6790 - LR: 1.0e-03\n",
      "Epoch 8/30 - Train Loss: 0.2146 - Val AUC: 0.6824 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6824, model saved.\n",
      "Epoch 9/30 - Train Loss: 0.2125 - Val AUC: 0.6799 - LR: 1.0e-03\n",
      "Epoch 10/30 - Train Loss: 0.2109 - Val AUC: 0.6792 - LR: 1.0e-03\n",
      "Epoch 11/30 - Train Loss: 0.2095 - Val AUC: 0.6747 - LR: 2.0e-04\n",
      "Epoch 12/30 - Train Loss: 0.2055 - Val AUC: 0.6757 - LR: 2.0e-04\n",
      "Epoch 13/30 - Train Loss: 0.2045 - Val AUC: 0.6749 - LR: 2.0e-04\n",
      "Epoch 14/30 - Train Loss: 0.2046 - Val AUC: 0.6752 - LR: 2.0e-04\n",
      "Epoch 15/30 - Train Loss: 0.2037 - Val AUC: 0.6745 - LR: 2.0e-04\n",
      "Epoch 16/30 - Train Loss: 0.2032 - Val AUC: 0.6737 - LR: 2.0e-04\n",
      "Epoch 17/30 - Train Loss: 0.2021 - Val AUC: 0.6731 - LR: 4.0e-05\n",
      "Epoch 18/30 - Train Loss: 0.2013 - Val AUC: 0.6731 - LR: 4.0e-05\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DL Folds:  20%|██        | 1/5 [02:12<08:50, 132.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DL Fold 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yishu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 0.2958 - Val AUC: 0.6529 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6529, model saved.\n",
      "Epoch 2/30 - Train Loss: 0.2266 - Val AUC: 0.6664 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6664, model saved.\n",
      "Epoch 3/30 - Train Loss: 0.2232 - Val AUC: 0.6660 - LR: 1.0e-03\n",
      "Epoch 4/30 - Train Loss: 0.2211 - Val AUC: 0.6774 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6774, model saved.\n",
      "Epoch 5/30 - Train Loss: 0.2193 - Val AUC: 0.6817 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6817, model saved.\n",
      "Epoch 6/30 - Train Loss: 0.2172 - Val AUC: 0.6827 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6827, model saved.\n",
      "Epoch 7/30 - Train Loss: 0.2158 - Val AUC: 0.6826 - LR: 1.0e-03\n",
      "Epoch 8/30 - Train Loss: 0.2142 - Val AUC: 0.6827 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6827, model saved.\n",
      "Epoch 9/30 - Train Loss: 0.2113 - Val AUC: 0.6807 - LR: 1.0e-03\n",
      "Epoch 10/30 - Train Loss: 0.2095 - Val AUC: 0.6795 - LR: 1.0e-03\n",
      "Epoch 11/30 - Train Loss: 0.2079 - Val AUC: 0.6784 - LR: 1.0e-03\n",
      "Epoch 12/30 - Train Loss: 0.2053 - Val AUC: 0.6748 - LR: 2.0e-04\n",
      "Epoch 13/30 - Train Loss: 0.2019 - Val AUC: 0.6748 - LR: 2.0e-04\n",
      "Epoch 14/30 - Train Loss: 0.2008 - Val AUC: 0.6738 - LR: 2.0e-04\n",
      "Epoch 15/30 - Train Loss: 0.2001 - Val AUC: 0.6722 - LR: 2.0e-04\n",
      "Epoch 16/30 - Train Loss: 0.1992 - Val AUC: 0.6716 - LR: 2.0e-04\n",
      "Epoch 17/30 - Train Loss: 0.1993 - Val AUC: 0.6695 - LR: 2.0e-04\n",
      "Epoch 18/30 - Train Loss: 0.1983 - Val AUC: 0.6693 - LR: 4.0e-05\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DL Folds:  40%|████      | 2/5 [04:30<06:47, 135.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DL Fold 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yishu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 0.3200 - Val AUC: 0.6630 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6630, model saved.\n",
      "Epoch 2/30 - Train Loss: 0.2270 - Val AUC: 0.6723 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6723, model saved.\n",
      "Epoch 3/30 - Train Loss: 0.2241 - Val AUC: 0.6776 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6776, model saved.\n",
      "Epoch 4/30 - Train Loss: 0.2220 - Val AUC: 0.6819 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6819, model saved.\n",
      "Epoch 5/30 - Train Loss: 0.2203 - Val AUC: 0.6832 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6832, model saved.\n",
      "Epoch 6/30 - Train Loss: 0.2179 - Val AUC: 0.6873 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6873, model saved.\n",
      "Epoch 7/30 - Train Loss: 0.2167 - Val AUC: 0.6870 - LR: 1.0e-03\n",
      "Epoch 8/30 - Train Loss: 0.2148 - Val AUC: 0.6882 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6882, model saved.\n",
      "Epoch 9/30 - Train Loss: 0.2135 - Val AUC: 0.6883 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6883, model saved.\n",
      "Epoch 10/30 - Train Loss: 0.2116 - Val AUC: 0.6869 - LR: 1.0e-03\n",
      "Epoch 11/30 - Train Loss: 0.2098 - Val AUC: 0.6873 - LR: 1.0e-03\n",
      "Epoch 12/30 - Train Loss: 0.2080 - Val AUC: 0.6863 - LR: 1.0e-03\n",
      "Epoch 13/30 - Train Loss: 0.2057 - Val AUC: 0.6858 - LR: 1.0e-03\n",
      "Epoch 14/30 - Train Loss: 0.2039 - Val AUC: 0.6850 - LR: 1.0e-03\n",
      "Epoch 15/30 - Train Loss: 0.2014 - Val AUC: 0.6777 - LR: 2.0e-04\n",
      "Epoch 16/30 - Train Loss: 0.1975 - Val AUC: 0.6789 - LR: 2.0e-04\n",
      "Epoch 17/30 - Train Loss: 0.1964 - Val AUC: 0.6785 - LR: 2.0e-04\n",
      "Epoch 18/30 - Train Loss: 0.1953 - Val AUC: 0.6770 - LR: 2.0e-04\n",
      "Epoch 19/30 - Train Loss: 0.1950 - Val AUC: 0.6767 - LR: 2.0e-04\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DL Folds:  60%|██████    | 3/5 [06:52<04:37, 138.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DL Fold 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yishu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 0.3040 - Val AUC: 0.6649 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6649, model saved.\n",
      "Epoch 2/30 - Train Loss: 0.2264 - Val AUC: 0.6719 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6719, model saved.\n",
      "Epoch 3/30 - Train Loss: 0.2236 - Val AUC: 0.6740 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6740, model saved.\n",
      "Epoch 4/30 - Train Loss: 0.2218 - Val AUC: 0.6808 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6808, model saved.\n",
      "Epoch 5/30 - Train Loss: 0.2199 - Val AUC: 0.6832 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6832, model saved.\n",
      "Epoch 6/30 - Train Loss: 0.2180 - Val AUC: 0.6866 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6866, model saved.\n",
      "Epoch 7/30 - Train Loss: 0.2169 - Val AUC: 0.6887 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6887, model saved.\n",
      "Epoch 8/30 - Train Loss: 0.2152 - Val AUC: 0.6860 - LR: 1.0e-03\n",
      "Epoch 9/30 - Train Loss: 0.2132 - Val AUC: 0.6860 - LR: 1.0e-03\n",
      "Epoch 10/30 - Train Loss: 0.2114 - Val AUC: 0.6863 - LR: 1.0e-03\n",
      "Epoch 11/30 - Train Loss: 0.2099 - Val AUC: 0.6819 - LR: 1.0e-03\n",
      "Epoch 12/30 - Train Loss: 0.2079 - Val AUC: 0.6820 - LR: 1.0e-03\n",
      "Epoch 13/30 - Train Loss: 0.2059 - Val AUC: 0.6782 - LR: 2.0e-04\n",
      "Epoch 14/30 - Train Loss: 0.2014 - Val AUC: 0.6792 - LR: 2.0e-04\n",
      "Epoch 15/30 - Train Loss: 0.2011 - Val AUC: 0.6769 - LR: 2.0e-04\n",
      "Epoch 16/30 - Train Loss: 0.1999 - Val AUC: 0.6768 - LR: 2.0e-04\n",
      "Epoch 17/30 - Train Loss: 0.1995 - Val AUC: 0.6762 - LR: 2.0e-04\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DL Folds:  80%|████████  | 4/5 [08:54<02:12, 132.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DL Fold 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yishu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 0.3313 - Val AUC: 0.6494 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6494, model saved.\n",
      "Epoch 2/30 - Train Loss: 0.2272 - Val AUC: 0.6646 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6646, model saved.\n",
      "Epoch 3/30 - Train Loss: 0.2240 - Val AUC: 0.6722 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6722, model saved.\n",
      "Epoch 4/30 - Train Loss: 0.2213 - Val AUC: 0.6754 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6754, model saved.\n",
      "Epoch 5/30 - Train Loss: 0.2199 - Val AUC: 0.6778 - LR: 1.0e-03\n",
      "  Best val_auc improved to 0.6778, model saved.\n",
      "Epoch 6/30 - Train Loss: 0.2184 - Val AUC: 0.6763 - LR: 1.0e-03\n",
      "Epoch 7/30 - Train Loss: 0.2162 - Val AUC: 0.6776 - LR: 1.0e-03\n",
      "Epoch 8/30 - Train Loss: 0.2150 - Val AUC: 0.6753 - LR: 1.0e-03\n",
      "Epoch 9/30 - Train Loss: 0.2130 - Val AUC: 0.6753 - LR: 1.0e-03\n",
      "Epoch 10/30 - Train Loss: 0.2109 - Val AUC: 0.6769 - LR: 1.0e-03\n",
      "Epoch 11/30 - Train Loss: 0.2099 - Val AUC: 0.6750 - LR: 2.0e-04\n",
      "Epoch 12/30 - Train Loss: 0.2061 - Val AUC: 0.6743 - LR: 2.0e-04\n",
      "Epoch 13/30 - Train Loss: 0.2054 - Val AUC: 0.6739 - LR: 2.0e-04\n",
      "Epoch 14/30 - Train Loss: 0.2039 - Val AUC: 0.6729 - LR: 2.0e-04\n",
      "Epoch 15/30 - Train Loss: 0.2037 - Val AUC: 0.6718 - LR: 2.0e-04\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DL Folds: 100%|██████████| 5/5 [10:43<00:00, 128.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall DL OOF AUC: 0.68362\n",
      "Ensembling LGBM and PyTorch DL predictions...\n",
      "LGBM OOF: 0.6869, DL OOF: 0.6836\n",
      "Ensemble Weights -> LGBM: 0.501, DL: 0.499\n",
      "Submission file 'prediction_pytorch_lgbm.csv' created with 261477 rows.\n",
      "Sample predictions:\n",
      "   user_id  merchant_id      prob\n",
      "0   163968         4605  0.073557\n",
      "1   360576         1581  0.115936\n",
      "2    98688         1964  0.077613\n",
      "3    98688         3645  0.069432\n",
      "4   295296         3361  0.084315\n",
      "Script finished at: 2025-05-11 23:08:53.383661. Total runtime: 0:20:10.547381\n"
     ]
    }
   ],
   "source": [
    "# --- Feature Engineering Functions ---\n",
    "def engineer_user_features(user_log, user_info):\n",
    "    \"\"\"Engineers features at the user level.\"\"\"\n",
    "    print(\"Engineering user-level features...\")\n",
    "    # begin with user demographic data\n",
    "    features = user_info.copy()\n",
    "    features.rename(columns={'age_range':'u_age_range', 'gender':'u_gender'}, inplace=True)\n",
    "    # those history of logs before D11\n",
    "    log_hist = user_log[user_log['days_before_d11'] > 0].copy()\n",
    "    # User calculations below\n",
    "    agg_funcs_hist = {\n",
    "        'item_id': ['count', 'nunique'], 'cat_id': ['nunique'], 'merchant_id': ['nunique'],\n",
    "        'brand_id': ['nunique'], 'days_before_d11': ['nunique', 'max', 'min', 'mean', 'std'], \n",
    "    }\n",
    "    # Group by user_id and aggregate\n",
    "    user_activity_stats_hist = log_hist.groupby('user_id').agg(agg_funcs_hist)\n",
    "    # Prefix the columns with 'u_hist_'\n",
    "    user_activity_stats_hist.columns = ['u_hist_' + '_'.join(col).strip() for col in user_activity_stats_hist.columns.values]\n",
    "    # Rename columns for clarity\n",
    "    user_activity_stats_hist.rename(columns={\n",
    "        'u_hist_item_id_count': 'u_hist_total_actions', 'u_hist_item_id_nunique': 'u_hist_n_distinct_items',\n",
    "        'u_hist_cat_id_nunique': 'u_hist_n_distinct_categories', 'u_hist_merchant_id_nunique': 'u_hist_n_distinct_merchants',\n",
    "        'u_hist_brand_id_nunique': 'u_hist_n_distinct_brands', 'u_hist_days_before_d11_nunique': 'u_hist_days_active',\n",
    "        'u_hist_days_before_d11_max': 'u_hist_earliest_action_days_prior', \n",
    "        'u_hist_days_before_d11_min': 'u_hist_latest_action_days_prior',\n",
    "        'u_hist_days_before_d11_mean': 'u_hist_mean_action_days_prior',\n",
    "        'u_hist_days_before_d11_std': 'u_hist_std_action_days_prior',\n",
    "    }, inplace=True)\n",
    "    # Merge the aggregated features with the main features DataFrame\n",
    "    features = features.merge(user_activity_stats_hist.reset_index(), on='user_id', how='left')\n",
    "    # Historical Action Type Counts:\n",
    "    action_type_counts_hist = log_hist.groupby(['user_id', 'action_type']).size().unstack(fill_value=0)\n",
    "    action_type_counts_hist.columns = [f'u_hist_action_type_{col}_count' for col in action_type_counts_hist.columns]\n",
    "    features = features.merge(action_type_counts_hist.reset_index(), on='user_id', how='left')\n",
    "    \n",
    "    for act_type in [0, 1, 2, 3]: \n",
    "        col_name = f'u_hist_action_type_{act_type}_count'\n",
    "        if col_name not in features.columns: features[col_name] = 0\n",
    "    # Historical Ratios:        \n",
    "    features['u_hist_purchase_to_click_ratio'] = features['u_hist_action_type_2_count'] / (features['u_hist_action_type_0_count'] + 1e-6)\n",
    "    features['u_hist_purchase_to_cart_ratio'] = features['u_hist_action_type_2_count'] / (features['u_hist_action_type_1_count'] + 1e-6)\n",
    "    features['u_hist_purchase_to_fav_ratio'] = features['u_hist_action_type_2_count'] / (features['u_hist_action_type_3_count'] + 1e-6)\n",
    "    features['u_hist_cart_to_click_ratio'] = features['u_hist_action_type_1_count'] / (features['u_hist_action_type_0_count'] + 1e-6)\n",
    "\n",
    "    log_d11 = user_log[user_log['is_d11'] == 1].copy()\n",
    "    user_d11_activity_counts = log_d11.groupby('user_id').agg(\n",
    "        u_d11_total_actions = ('item_id', 'count'), u_d11_n_distinct_items = ('item_id', 'nunique'),\n",
    "        u_d11_n_distinct_merchants = ('merchant_id', 'nunique'), u_d11_n_distinct_cats = ('cat_id', 'nunique')\n",
    "    ).reset_index()\n",
    "    features = features.merge(user_d11_activity_counts, on='user_id', how='left')\n",
    "    # \"Double 11\" General Activity (User's overall activity on D11):\n",
    "    action_type_counts_d11 = log_d11.groupby(['user_id', 'action_type']).size().unstack(fill_value=0)\n",
    "    action_type_counts_d11.columns = [f'u_d11_action_type_{col}_count' for col in action_type_counts_d11.columns]\n",
    "    features = features.merge(action_type_counts_d11.reset_index(), on='user_id', how='left')\n",
    "    # Temporal Window Features (Historical): window actions before D11\n",
    "    for days_window in [1, 3, 7, 15, 30, 60, 90, 180]:\n",
    "        temp_log_window = log_hist[log_hist['days_before_d11'] <= days_window]\n",
    "        user_window_actions = temp_log_window.groupby('user_id')['item_id'].count().reset_index(name=f'u_actions_last_{days_window}d_prior')\n",
    "        features = features.merge(user_window_actions, on='user_id', how='left')\n",
    "        user_window_purchases = temp_log_window[temp_log_window['action_type']==2].groupby('user_id')['item_id'].count().reset_index(name=f'u_purchases_last_{days_window}d_prior')\n",
    "        features = features.merge(user_window_purchases, on='user_id', how='left')\n",
    "    return features.fillna(0)\n",
    "# fill nan to 0 if possible\n",
    "def engineer_merchant_features(user_log):\n",
    "    print(\"Engineering merchant-level features...\")\n",
    "    #Historical Popularity (Before \"Double 11\"):\n",
    "\n",
    "    #How many interactions the merchant received in total.\n",
    "    #How many unique users interacted with them.\n",
    "    #How many different items, brands, and categories they handled.\n",
    "    #Counts of different action types (clicks, purchases, etc.) directed at them.\n",
    "    #Their historical conversion rate (purchases / clicks).\n",
    "    #\"Double 11\" Activity:\n",
    "\n",
    "    #How many interactions they received on \"Double 11\".\n",
    "    #How many unique users interacted with them on \"Double 11\".\n",
    "    #Counts of different action types they received on \"Double 11\".\n",
    "    unique_merchants = user_log['merchant_id'].unique()\n",
    "    features = pd.DataFrame({'merchant_id': unique_merchants[pd.notna(unique_merchants)]})\n",
    "    log_hist = user_log[user_log['days_before_d11'] > 0].copy()\n",
    "    agg_funcs_m_hist = {'user_id': ['count', 'nunique'], 'item_id': ['nunique'], 'brand_id': ['nunique'], 'cat_id': ['nunique']}\n",
    "    merchant_stats_hist = log_hist.groupby('merchant_id').agg(agg_funcs_m_hist)\n",
    "    merchant_stats_hist.columns = ['m_hist_' + '_'.join(col).strip() for col in merchant_stats_hist.columns.values]\n",
    "    merchant_stats_hist.rename(columns={\n",
    "        'm_hist_user_id_count': 'm_hist_total_interactions_received', 'm_hist_user_id_nunique': 'm_hist_n_distinct_users',\n",
    "        'm_hist_item_id_nunique': 'm_hist_n_distinct_items_handled', 'm_hist_brand_id_nunique': 'm_hist_n_distinct_brands_handled',\n",
    "        'm_hist_cat_id_nunique': 'm_hist_n_distinct_categories_handled'}, inplace=True)\n",
    "    features = features.merge(merchant_stats_hist.reset_index(), on='merchant_id', how='left')\n",
    "\n",
    "    merchant_action_counts_hist = log_hist.groupby(['merchant_id', 'action_type']).size().unstack(fill_value=0)\n",
    "    merchant_action_counts_hist.columns = [f'm_hist_action_type_{col}_count' for col in merchant_action_counts_hist.columns]\n",
    "    features = features.merge(merchant_action_counts_hist.reset_index(), on='merchant_id', how='left')\n",
    "    for act_type in [0, 1, 2, 3]: \n",
    "        col_name = f'm_hist_action_type_{act_type}_count'\n",
    "        if col_name not in features.columns: features[col_name] = 0\n",
    "    features['m_hist_conversion_rate'] = features['m_hist_action_type_2_count'] / (features['m_hist_action_type_0_count'] + 1e-6)\n",
    "\n",
    "    log_d11 = user_log[user_log['is_d11'] == 1].copy()\n",
    "    merchant_d11_activity_counts = log_d11.groupby('merchant_id').agg(\n",
    "        m_d11_total_interactions = ('item_id', 'count'), m_d11_n_distinct_users = ('user_id', 'nunique'),\n",
    "        m_d11_n_distinct_items = ('item_id', 'nunique')).reset_index()\n",
    "    features = features.merge(merchant_d11_activity_counts, on='merchant_id', how='left')\n",
    "    merchant_d11_action_counts = log_d11.groupby(['merchant_id', 'action_type']).size().unstack(fill_value=0)\n",
    "    merchant_d11_action_counts.columns = [f'm_d11_action_type_{col}_count' for col in merchant_d11_action_counts.columns]\n",
    "    features = features.merge(merchant_d11_action_counts.reset_index(), on='merchant_id', how='left')\n",
    "    return features.fillna(0)\n",
    "\n",
    "def engineer_user_merchant_interaction_features(user_log, base_df):\n",
    "    print(\"Engineering user-merchant interaction features...\")\n",
    "    # user-merchant interaction features before D11\n",
    "    log_hist = user_log[user_log['days_before_d11'] > 0].copy()\n",
    "    um_interactions_hist_agg = log_hist.groupby(['user_id', 'merchant_id']).agg(\n",
    "        um_hist_total_actions=('item_id', 'count'), um_hist_distinct_items=('item_id', 'nunique'),\n",
    "        um_hist_distinct_cats=('cat_id', 'nunique'), um_hist_distinct_brands=('brand_id', 'nunique'),\n",
    "        um_hist_last_interaction_days_prior=('days_before_d11', 'min'),\n",
    "        um_hist_first_interaction_days_prior=('days_before_d11', 'max'),\n",
    "        um_hist_days_active_with_merchant=('days_before_d11', 'nunique')).reset_index()\n",
    "    merged_df = base_df.merge(um_interactions_hist_agg, on=['user_id', 'merchant_id'], how='left')\n",
    "    \n",
    "    um_action_counts_hist = log_hist.groupby(['user_id', 'merchant_id', 'action_type']).size().unstack(fill_value=0)\n",
    "    um_action_counts_hist.columns = [f'um_hist_action_type_{col}_count' for col in um_action_counts_hist.columns]\n",
    "    merged_df = merged_df.merge(um_action_counts_hist.reset_index(), on=['user_id', 'merchant_id'], how='left')\n",
    "    # user-merchant interaction features on D11\n",
    "    log_d11 = user_log[user_log['is_d11'] == 1].copy()\n",
    "    um_d11_interactions_agg = log_d11.groupby(['user_id', 'merchant_id']).agg(\n",
    "        um_d11_total_actions=('item_id', 'count'), um_d11_purchases=('action_type', lambda x: (x == 2).sum()),\n",
    "        um_d11_clicks=('action_type', lambda x: (x == 0).sum()), um_d11_carts=('action_type', lambda x: (x == 1).sum()),\n",
    "        um_d11_favs=('action_type', lambda x: (x == 3).sum()), \n",
    "        um_d11_distinct_items_interacted=('item_id', 'nunique')).reset_index()\n",
    "    merged_df = merged_df.merge(um_d11_interactions_agg, on=['user_id', 'merchant_id'], how='left')\n",
    "    # more weight to recent interactions\n",
    "    decay_rate = 0.01 \n",
    "    log_hist.loc[:, 'interaction_weight'] = np.exp(-decay_rate * log_hist['days_before_d11'])\n",
    "    um_time_decayed_score = log_hist.groupby(['user_id', 'merchant_id'])['interaction_weight'].sum().reset_index(name='um_hist_time_decayed_score')\n",
    "    merged_df = merged_df.merge(um_time_decayed_score, on=['user_id', 'merchant_id'], how='left')\n",
    "    # first item the user purchased from this specific merchant on \"Double 11\", what kind of product initiated the \"new buyer\" relationship.\n",
    "    d11_purchases = log_d11[log_d11['action_type'] == 2]\n",
    "    first_d11_purchase_details = d11_purchases.drop_duplicates(subset=['user_id', 'merchant_id'], keep='first')\n",
    "    acquisition_item_features = first_d11_purchase_details[['user_id', 'merchant_id', 'item_id', 'cat_id', 'brand_id']]\n",
    "    acquisition_item_features.columns = ['user_id', 'merchant_id', 'acq_item_id', 'acq_cat_id', 'acq_brand_id']\n",
    "    merged_df = merged_df.merge(acquisition_item_features, on=['user_id', 'merchant_id'], how='left')\n",
    "    # no prior interactions with a merchant, or no \"Double 11\" interaction\n",
    "    interaction_cols_to_fill = [col for col in merged_df.columns if col.startswith('um_') or col.startswith('acq_')]\n",
    "    for col in interaction_cols_to_fill:\n",
    "        if 'days_prior' in col or 'score' in col: merged_df[col].fillna(-1, inplace=True)\n",
    "        elif col in ['acq_item_id', 'acq_cat_id', 'acq_brand_id']: merged_df[col].fillna(0, inplace=True)\n",
    "        else: merged_df[col].fillna(0, inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "def create_all_features(user_log, user_info, train_data, test_data):\n",
    "    print(\"Starting comprehensive feature engineering...\")\n",
    "    train_ids_df = train_data[['user_id', 'merchant_id']].copy()\n",
    "    test_ids_df = test_data[['user_id', 'merchant_id']].copy()\n",
    "    train_ids_df['_is_train_data_source'] = 1\n",
    "    test_ids_df['_is_train_data_source'] = 0\n",
    "    all_pairs_df = pd.concat([train_ids_df, test_ids_df], axis=0).drop_duplicates(subset=['user_id', 'merchant_id'])\n",
    "    # combine train and test targets\n",
    "    df_user_features = engineer_user_features(user_log, user_info)\n",
    "    all_pairs_featured_df = all_pairs_df.merge(df_user_features, on='user_id', how='left')\n",
    "    del df_user_features; gc.collect()\n",
    "    # all general user characteristics\n",
    "    df_merchant_features = engineer_merchant_features(user_log)\n",
    "    all_pairs_featured_df = all_pairs_featured_df.merge(df_merchant_features, on='merchant_id', how='left')\n",
    "    del df_merchant_features; gc.collect()\n",
    "    # all general merchant characteristics\n",
    "    all_pairs_featured_df = engineer_user_merchant_interaction_features(user_log, all_pairs_featured_df)\n",
    "    # how specific user interacted with specific merchant\n",
    "    train_featured_df = all_pairs_featured_df[all_pairs_featured_df['_is_train_data_source'] == 1].drop(columns=['_is_train_data_source'])\n",
    "    test_featured_df = all_pairs_featured_df[all_pairs_featured_df['_is_train_data_source'] == 0].drop(columns=['_is_train_data_source'])\n",
    "    \n",
    "    train_featured_df = train_featured_df.merge(train_data[['user_id', 'merchant_id', 'label']], on=['user_id', 'merchant_id'], how='left')\n",
    "    test_featured_df = test_data[['user_id', 'merchant_id', 'prob']].merge(test_featured_df.drop(columns=['prob'], errors='ignore'), on=['user_id', 'merchant_id'], how='left')\n",
    "    # Split the data into fully featured train and test sets\n",
    "    ratio_feature_defs = [\n",
    "        ('um_hist_total_actions', 'u_hist_total_actions', 'ratio_um_hist_actions_vs_u_hist_actions'),\n",
    "        ('um_d11_total_actions', 'u_d11_total_actions', 'ratio_um_d11_actions_vs_u_d11_actions'),\n",
    "        ('um_d11_purchases', 'u_d11_action_type_2_count', 'ratio_um_d11_purchases_vs_u_d11_purchases'),\n",
    "        ('um_d11_total_actions', 'm_d11_total_interactions', 'ratio_um_d11_actions_vs_m_d11_interactions'),\n",
    "    ]\n",
    "    for df_iter in [train_featured_df, test_featured_df]:\n",
    "        for num_col, den_col, ratio_col_name in ratio_feature_defs:\n",
    "            if num_col in df_iter.columns and den_col in df_iter.columns:\n",
    "                df_iter[ratio_col_name] = df_iter[num_col] / (df_iter[den_col] + 1e-6)\n",
    "            else:\n",
    "                df_iter[ratio_col_name] = 0 \n",
    "    train_featured_df.fillna(0, inplace=True)\n",
    "    test_featured_df.fillna(0, inplace=True)\n",
    "    # calculate the ratios and deal with the missing values\n",
    "    print(\"Feature engineering complete.\")\n",
    "    return train_featured_df, test_featured_df\n",
    "\n",
    "# --- Model Training (LightGBM) ---\n",
    "def train_predict_lgbm(train_df, test_df, features_to_use, target_col='label', n_splits=5):\n",
    "    print(\"Training LightGBM model...\")\n",
    "    # Separate train and test data\n",
    "    X = train_df[features_to_use].copy()\n",
    "    y = train_df[target_col]\n",
    "    X_test = test_df[features_to_use].copy()\n",
    "    \n",
    "    oof_preds = np.zeros(X.shape[0])\n",
    "    test_preds = np.zeros(X_test.shape[0])\n",
    "    # Convert categorical features to 'category' dtype for LightGBM\n",
    "    categorical_feature_names = ['u_age_range', 'u_gender', 'merchant_id', 'acq_item_id', 'acq_cat_id', 'acq_brand_id']\n",
    "    categorical_features_for_lgbm = [f for f in categorical_feature_names if f in features_to_use]\n",
    "    for col in categorical_features_for_lgbm:\n",
    "        if col in X.columns: X.loc[:, col] = X[col].astype('category')\n",
    "        if col in X_test.columns: X_test.loc[:, col] = X_test[col].astype('category')\n",
    "    # Stratified K-Fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Initialize LightGBM parameters\n",
    "    params = {\n",
    "        'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt',\n",
    "        'n_estimators': 3000, 'learning_rate': 0.01, 'num_leaves': 42, 'max_depth': 7,\n",
    "        'seed': 42, 'n_jobs': -1, 'verbose': -1, 'colsample_bytree': 0.7, 'subsample': 0.7,\n",
    "        'subsample_freq': 1, 'reg_alpha': 0.15, 'reg_lambda': 0.15,\n",
    "    }\n",
    "    # DataFrame to store feature importances\n",
    "    feature_importances_df = pd.DataFrame(index=features_to_use)\n",
    "    # Training loop and predictions\n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X, y), total=n_splits, desc=\"LGBM Folds\")):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='auc',\n",
    "                  callbacks=[lgb.early_stopping(150, verbose=False)],\n",
    "                  categorical_feature=categorical_features_for_lgbm if categorical_features_for_lgbm else 'auto')\n",
    "        oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "        test_preds += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "        feature_importances_df[f'fold_{fold+1}'] = pd.Series(model.feature_importances_, index=features_to_use)\n",
    "    # report the OOF AUC performance\n",
    "    oof_auc = roc_auc_score(y, oof_preds)\n",
    "    print(f\"LGBM OOF AUC: {oof_auc:.5f}\")\n",
    "    feature_importances_df['mean_importance'] = feature_importances_df.mean(axis=1)\n",
    "    feature_importances_df.sort_values(by='mean_importance', ascending=False, inplace=True)\n",
    "    print(\"\\nLGBM Top 30 Feature Importances (Mean over folds):\")\n",
    "    print(feature_importances_df[['mean_importance']].head(30))\n",
    "    return test_preds, oof_auc, feature_importances_df\n",
    "\n",
    "# --- Deep Learning Model (PyTorch) ---\n",
    "if PYTORCH_AVAILABLE:\n",
    "    # This is a helper to organize the data (categorical features, numerical features, and the target labels)\n",
    "    #  in a way PyTorch can easily use, especially for feeding data in batches during training.\n",
    "    class TianchiDataset(Dataset):\n",
    "        \"\"\"Custom PyTorch Dataset for handling mixed data types.\"\"\"\n",
    "        def __init__(self, cat_features, num_features, labels=None):\n",
    "            self.cat_features = {k: torch.tensor(v, dtype=torch.long) for k, v in cat_features.items()}\n",
    "            self.num_features = torch.tensor(num_features, dtype=torch.float32)\n",
    "            self.labels = torch.tensor(labels, dtype=torch.float32) if labels is not None else None\n",
    "\n",
    "        def __len__(self):\n",
    "            # Assume all categorical features have the same length, pick one\n",
    "            return len(self.num_features)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            cat_item = {k: v[idx] for k, v in self.cat_features.items()}\n",
    "            num_item = self.num_features[idx]\n",
    "            if self.labels is not None:\n",
    "                return (cat_item, num_item), self.labels[idx].unsqueeze(-1) # Ensure label is [batch_size, 1]\n",
    "            else:\n",
    "                return (cat_item, num_item)\n",
    "\n",
    "    class DeepNet(nn.Module):\n",
    "        # This class builds the actual neural network.\n",
    "        \"\"\"PyTorch Deep Learning Model for tabular data.\"\"\"\n",
    "        # It takes categorical features (like merchant_id, age_range) and turns them into dense vector representations called \"embeddings.\" This helps the model understand relationships between different categories.\n",
    "        def __init__(self, embedding_info, num_numerical_features, hidden_dims=[512, 256, 128], dropout_rates=[0.4, 0.4, 0.3]):\n",
    "            super(DeepNet, self).__init__()\n",
    "            self.embeddings = nn.ModuleList()\n",
    "            total_embedding_dim = 0\n",
    "            for col_name, input_dim, output_dim in embedding_info:\n",
    "                self.embeddings.append(nn.Embedding(input_dim, output_dim))\n",
    "                total_embedding_dim += output_dim\n",
    "            \n",
    "            self.embedding_dropout = nn.Dropout(0.2) # Dropout after embedding concatenation\n",
    "            # It then combines these learned embeddings with the regular numerical features.\n",
    "            # Dense layers\n",
    "            all_input_dims = total_embedding_dim + num_numerical_features\n",
    "            # This combined information is passed through several \"dense layers\" (standard neural network layers) with techniques like BatchNormalization (to stabilize learning) and Dropout (to prevent overfitting).\n",
    "            layers = []\n",
    "            for i, hidden_dim in enumerate(hidden_dims):\n",
    "                layers.append(nn.Linear(all_input_dims if i == 0 else hidden_dims[i-1], hidden_dim))\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout_rates[i]))\n",
    "            \n",
    "            self.dense_layers = nn.Sequential(*layers)\n",
    "            # The last layer outputs a single probability (between 0 and 1) that the user will be loyal.\n",
    "            self.output_layer = nn.Linear(hidden_dims[-1] if hidden_dims else all_input_dims, 1)\n",
    "\n",
    "        def forward(self, x_cat, x_num):\n",
    "            embedded_cats = []\n",
    "            # x_cat is a dictionary: {'col_name': tensor_data, ...}\n",
    "            # self.embeddings is a ModuleList, need to iterate carefully or name them\n",
    "            # Assuming embedding_info provides names in the same order as self.embeddings\n",
    "            for i, col_name in enumerate(x_cat.keys()): # Iterate through input categorical feature names\n",
    "                 embedded_cats.append(self.embeddings[i](x_cat[col_name]))\n",
    "            \n",
    "            if embedded_cats:\n",
    "                embedded_cats_concat = torch.cat(embedded_cats, dim=1)\n",
    "                embedded_cats_concat = self.embedding_dropout(embedded_cats_concat)\n",
    "                x = torch.cat([embedded_cats_concat, x_num], dim=1)\n",
    "            else:\n",
    "                x = x_num\n",
    "                \n",
    "            x = self.dense_layers(x)\n",
    "            x = torch.sigmoid(self.output_layer(x))\n",
    "            return x\n",
    "\n",
    "    def train_predict_deep_model(train_df, test_df, categorical_cols_embed, numerical_cols, target_col='label', n_splits=5, epochs=50, batch_size=1024):\n",
    "        if not PYTORCH_AVAILABLE:\n",
    "            print(\"PyTorch not available. Skipping Deep Learning model.\")\n",
    "            return np.zeros(len(test_df)), 0.0\n",
    "\n",
    "        print(\"Preparing data for PyTorch Deep Learning model...\")\n",
    "        # Data preparation for PyTorch\n",
    "        # Store encoders and embedding info globally for consistent test set transformation\n",
    "        label_encoders = {}\n",
    "        embedding_info_list = [] \n",
    "\n",
    "        # Prepare categorical features for PyTorch\n",
    "        X_cat_train_processed = {}\n",
    "        X_cat_test_processed = {}\n",
    "\n",
    "        for col in tqdm(categorical_cols_embed, desc=\"Label Encoding DL Categoricals\"):\n",
    "            # Combine train and test for fitting encoder to see all possible values\n",
    "            combined_data = pd.concat([train_df[col], test_df[col]], axis=0).astype(str).fillna('__MISSING__')\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(combined_data)\n",
    "            label_encoders[col] = encoder # Store encoder\n",
    "\n",
    "            X_cat_train_processed[col] = encoder.transform(train_df[col].astype(str).fillna('__MISSING__'))\n",
    "            X_cat_test_processed[col] = encoder.transform(test_df[col].astype(str).fillna('__MISSING__'))\n",
    "            \n",
    "            input_dim = len(encoder.classes_) # Number of unique categories\n",
    "            output_dim = min(50, (input_dim + 1) // 2) # Heuristic for embedding output dimension\n",
    "            embedding_info_list.append((col, input_dim, output_dim))\n",
    "            print(f\"  DL Cat Feature: {col}, Input Dim: {input_dim}, Output Dim: {output_dim}\")\n",
    "\n",
    "        y_train_dl = train_df[target_col].values\n",
    "        oof_preds_dl = np.zeros(len(train_df))\n",
    "        test_preds_dl = np.zeros(len(test_df))\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        # Stratified K-Fold for Deep Learning\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=123)\n",
    "        # Training loop\n",
    "        for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(train_df, y_train_dl), total=n_splits, desc=\"DL Folds\")):\n",
    "            print(f\"--- DL Fold {fold+1}/{n_splits} ---\")\n",
    "            \n",
    "            # Numerical features scaling for this fold\n",
    "            scaler = StandardScaler()\n",
    "            X_num_train_fold_scaled = scaler.fit_transform(train_df.iloc[train_idx][numerical_cols].astype(np.float32))\n",
    "            X_num_val_fold_scaled = scaler.transform(train_df.iloc[val_idx][numerical_cols].astype(np.float32))\n",
    "            X_num_test_fold_scaled = scaler.transform(test_df[numerical_cols].astype(np.float32)) # Scale test set\n",
    "\n",
    "            # Prepare Keras inputs for this fold\n",
    "            fold_X_cat_train = {col: X_cat_train_processed[col][train_idx] for col in categorical_cols_embed}\n",
    "            fold_X_cat_val = {col: X_cat_train_processed[col][val_idx] for col in categorical_cols_embed}\n",
    "            \n",
    "            train_dataset = TianchiDataset(fold_X_cat_train, X_num_train_fold_scaled, y_train_dl[train_idx])\n",
    "            val_dataset = TianchiDataset(fold_X_cat_val, X_num_val_fold_scaled, y_train_dl[val_idx])\n",
    "            # Create DataLoader for batching\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size * 2, shuffle=False)\n",
    "            # Initialize the model, optimizer, and loss function (Adam optimizer and binary cross-entropy loss)\n",
    "            model = DeepNet(embedding_info_list, len(numerical_cols)).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            criterion = nn.BCELoss() # Binary Cross Entropy for binary classification\n",
    "\n",
    "            best_val_auc = -1\n",
    "            patience_counter = 0\n",
    "            patience_epochs = 10 # For early stopping\n",
    "            # lower the learning rate if no improvement in validation AUC\n",
    "            # This is similar to ReduceLROnPlateau in Keras, but we will use a custom scheduler\n",
    "            # ReduceLROnPlateau equivalent\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=5, verbose=True, min_lr=1e-6)\n",
    "            # Early stopping based on validation AUC\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                train_loss_epoch = 0\n",
    "                for (cat_batch, num_batch), labels_batch in train_loader:\n",
    "                    cat_batch = {k: v.to(device) for k,v in cat_batch.items()}\n",
    "                    num_batch, labels_batch = num_batch.to(device), labels_batch.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(cat_batch, num_batch)\n",
    "                    loss = criterion(outputs, labels_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_loss_epoch += loss.item()\n",
    "                \n",
    "                model.eval()\n",
    "                val_preds_epoch = []\n",
    "                val_labels_epoch = []\n",
    "                with torch.no_grad():\n",
    "                    for (cat_batch, num_batch), labels_batch in val_loader:\n",
    "                        cat_batch = {k: v.to(device) for k,v in cat_batch.items()}\n",
    "                        num_batch = num_batch.to(device)\n",
    "                        outputs = model(cat_batch, num_batch)\n",
    "                        val_preds_epoch.extend(outputs.cpu().numpy().ravel())\n",
    "                        val_labels_epoch.extend(labels_batch.cpu().numpy().ravel())\n",
    "                \n",
    "                current_val_auc = roc_auc_score(val_labels_epoch, val_preds_epoch)\n",
    "                scheduler.step(current_val_auc) # For ReduceLROnPlateau\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss_epoch/len(train_loader):.4f} - Val AUC: {current_val_auc:.4f} - LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "                if current_val_auc > best_val_auc:\n",
    "                    best_val_auc = current_val_auc\n",
    "                    torch.save(model.state_dict(), DL_MODEL_CHECKPOINT_PATH.format(fold=fold+1))\n",
    "                    print(f\"  Best val_auc improved to {best_val_auc:.4f}, model saved.\")\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience_epochs:\n",
    "                    print(\"  Early stopping triggered.\")\n",
    "                    break\n",
    "            \n",
    "            # Load best model for OOF and test predictions\n",
    "            model.load_state_dict(torch.load(DL_MODEL_CHECKPOINT_PATH.format(fold=fold+1)))\n",
    "            model.eval()\n",
    "            # Out-of-Fold predictions for this fold\n",
    "            val_preds_list = []\n",
    "            with torch.no_grad():\n",
    "                for (cat_batch, num_batch), _ in val_loader: # Use val_loader again for consistency\n",
    "                    cat_batch = {k: v.to(device) for k,v in cat_batch.items()}\n",
    "                    num_batch = num_batch.to(device)\n",
    "                    outputs = model(cat_batch, num_batch)\n",
    "                    val_preds_list.extend(outputs.cpu().numpy().ravel())\n",
    "            oof_preds_dl[val_idx] = val_preds_list[:len(val_idx)] # Ensure correct length\n",
    "\n",
    "            # Test predictions for this fold\n",
    "            # Do the averaging across folds\n",
    "            fold_test_cat_inputs = {col: X_cat_test_processed[col] for col in categorical_cols_embed}\n",
    "            test_dataset_fold = TianchiDataset(fold_test_cat_inputs, X_num_test_fold_scaled) # No labels for test\n",
    "            test_loader_fold = DataLoader(test_dataset_fold, batch_size=batch_size*2, shuffle=False)\n",
    "            \n",
    "            current_fold_test_preds = []\n",
    "            with torch.no_grad():\n",
    "                for (cat_batch, num_batch) in test_loader_fold:\n",
    "                    cat_batch = {k: v.to(device) for k,v in cat_batch.items()}\n",
    "                    num_batch = num_batch.to(device)\n",
    "                    outputs = model(cat_batch, num_batch)\n",
    "                    current_fold_test_preds.extend(outputs.cpu().numpy().ravel())\n",
    "            test_preds_dl += np.array(current_fold_test_preds) / n_splits\n",
    "            \n",
    "            del model, train_loader, val_loader, test_loader_fold, X_num_train_fold_scaled, X_num_val_fold_scaled, X_num_test_fold_scaled\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        # Final output predictions and overall OOF AUC\n",
    "        overall_oof_auc_dl = roc_auc_score(y_train_dl, oof_preds_dl)\n",
    "        print(f\"Overall DL OOF AUC: {overall_oof_auc_dl:.5f}\")\n",
    "        return test_preds_dl, overall_oof_auc_dl\n",
    "else: # PYTORCH_AVAILABLE is False\n",
    "    def train_predict_deep_model(*args, **kwargs): # Stub if PyTorch not available\n",
    "        print(\"PyTorch is not installed. Skipping Deep Learning model training.\")\n",
    "        test_df_len = kwargs.get('test_df', pd.DataFrame()).shape[0] # Get length of test_df if passed\n",
    "        if 'train_df' in kwargs: test_df_len = kwargs['train_df'].shape[0] # Fallback for OOF shape\n",
    "        return np.zeros(test_df_len), 0.0\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # start, and time the script\n",
    "    script_start_time = datetime.now()\n",
    "    print(f\"Competition script started at: {script_start_time}\")\n",
    "    # Load data\n",
    "    user_log_df, user_info_df, train_target_df, test_target_df = load_data()\n",
    "    # Create Features with the function above\n",
    "    train_featured_df, test_featured_df = create_all_features(\n",
    "        user_log_df, user_info_df, train_target_df, test_target_df\n",
    "    )\n",
    "    del user_log_df, user_info_df; gc.collect()\n",
    "\n",
    "    print(f\"Train featured shape: {train_featured_df.shape}\")\n",
    "    print(f\"Test featured shape: {test_featured_df.shape}\")\n",
    "    # Define the target column and drop unnecessary columns for model definition\n",
    "    label_col = 'label'\n",
    "    cols_to_drop_for_model_definition = [label_col, 'prob', 'user_id'] \n",
    "    all_engineered_cols = [col for col in train_featured_df.columns if col not in cols_to_drop_for_model_definition]\n",
    "    \n",
    "    # Define categorical and numerical features for DL\n",
    "    # These lists should be carefully curated based on feature understanding\n",
    "    potential_cat_cols_for_dl = ['u_age_range', 'u_gender', 'merchant_id', \n",
    "                                 'acq_item_id', 'acq_cat_id', 'acq_brand_id'] \n",
    "    # Add more if they are truly categorical and suitable for embeddings\n",
    "    # e.g., if action type counts are binned or treated as categories.\n",
    "    # Separate categorical and numerical features\n",
    "    categorical_features_for_dl_embed = [col for col in potential_cat_cols_for_dl if col in all_engineered_cols]\n",
    "    numerical_features_for_dl = [col for col in all_engineered_cols if col not in categorical_features_for_dl_embed]\n",
    "\n",
    "    print(f\"Identified {len(categorical_features_for_dl_embed)} categorical features for DL embeddings: {categorical_features_for_dl_embed}\")\n",
    "    print(f\"Identified {len(numerical_features_for_dl)} numerical features for DL: {numerical_features_for_dl[:10]}...\")\n",
    "    # Train LightGBM and do predictions\n",
    "    # --- LightGBM Model ---\n",
    "    lgbm_features_to_use = all_engineered_cols \n",
    "    print(f\"Using {len(lgbm_features_to_use)} features for LGBM training.\")\n",
    "    \n",
    "    lgbm_test_preds, lgbm_oof_auc, _ = train_predict_lgbm(\n",
    "        train_featured_df.copy(), \n",
    "        test_featured_df.copy(),\n",
    "        lgbm_features_to_use,\n",
    "        target_col=label_col\n",
    "    )\n",
    "    # Train PyTorch Deep Learning model and do predictions\n",
    "    # --- Deep Learning Model (PyTorch) ---\n",
    "    dl_test_preds = None\n",
    "    dl_oof_auc = 0.0 # Default if DL is skipped\n",
    "    if PYTORCH_AVAILABLE:\n",
    "        if not train_featured_df.empty and not test_featured_df.empty and \\\n",
    "           (len(numerical_features_for_dl) > 0 or len(categorical_features_for_dl_embed) > 0) : # Ensure there are features\n",
    "            \n",
    "            dl_test_preds, dl_oof_auc = train_predict_deep_model(\n",
    "                train_featured_df,\n",
    "                test_featured_df,\n",
    "                categorical_features_for_dl_embed,\n",
    "                numerical_features_for_dl,\n",
    "                target_col=label_col,\n",
    "                n_splits=5, \n",
    "                epochs=30, # Adjust epochs based on observed convergence\n",
    "                batch_size=2048 # Adjust batch size based on memory and dataset size\n",
    "            )\n",
    "        else:\n",
    "            print(\"Skipping DL model due to no features or empty dataframes.\")\n",
    "    # Ensemble two models and do predictions by weighted average based on two OOF AUC\n",
    "    # --- Ensemble Predictions ---\n",
    "    if dl_test_preds is not None and lgbm_test_preds is not None:\n",
    "        print(\"Ensembling LGBM and PyTorch DL predictions...\")\n",
    "        # Simple average or weighted average based on OOF scores\n",
    "        # Example: Weighted average, tune weights based on OOF scores\n",
    "        total_oof_auc = lgbm_oof_auc + dl_oof_auc\n",
    "        if total_oof_auc > 0:\n",
    "            lgbm_weight = lgbm_oof_auc / total_oof_auc\n",
    "            dl_weight = dl_oof_auc / total_oof_auc\n",
    "        else: # Fallback if OOF AUCs are zero (e.g., if models failed or data is problematic)\n",
    "            lgbm_weight = 0.5\n",
    "            dl_weight = 0.5\n",
    "            \n",
    "        print(f\"LGBM OOF: {lgbm_oof_auc:.4f}, DL OOF: {dl_oof_auc:.4f}\")\n",
    "        print(f\"Ensemble Weights -> LGBM: {lgbm_weight:.3f}, DL: {dl_weight:.3f}\")\n",
    "        final_preds = (lgbm_weight * lgbm_test_preds) + (dl_weight * dl_test_preds)\n",
    "    elif lgbm_test_preds is not None:\n",
    "        print(\"Using only LGBM predictions.\")\n",
    "        final_preds = lgbm_test_preds\n",
    "    else:\n",
    "        print(\"No model predictions available. Generating dummy submission (all zeros).\")\n",
    "        final_preds = np.zeros(len(test_target_df)) \n",
    "    # Save the final predictions to a CSV file\n",
    "    # --- Create Submission File ---\n",
    "    submission_df = test_target_df[['user_id', 'merchant_id']].copy()\n",
    "    submission_df['prob'] = final_preds\n",
    "    submission_df['prob'] = np.clip(submission_df['prob'], 0.0, 1.0) \n",
    "    submission_df.to_csv(SUBMISSION_FILE, index=False, header=True) \n",
    "    print(f\"Submission file '{SUBMISSION_FILE}' created with {len(submission_df)} rows.\")\n",
    "    print(f\"Sample predictions:\\n{submission_df.head()}\")\n",
    "\n",
    "    script_end_time = datetime.now()\n",
    "    print(f\"Script finished at: {script_end_time}. Total runtime: {script_end_time - script_start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
